{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markdown的部分都已经转移到dorado上完成，此处只负责训练部分，如果要更改数据选取，取消Markdown后更改\n",
    "\n",
    "#!pip3 install -U http://tosv.byted.org/obj/turing-us/byted_turing-0.3.0-py3-none-any.whl\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Embedding, Input\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import random as rd\n",
    "from math import floor\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix"
   ]
  },
  {
   "source": [
    "#读取要分析的事件和字段\n",
    "events=lark_sdk.read_excel(url='https://bytedance.feishu.cn/sheets/shtcnZ4iIvOh7UmEO1OYuY39gDe?sheet=EqrTnO', sheet='Sheet12',header=True).values.tolist()\n",
    "\n",
    "#解析字段并分类\n",
    "usu=set(events[0][1].split('\\n'))\n",
    "unu=set(events[1][1].split('\\n'))\n",
    "eve=[[i[0],set(i[1].split('\\n')).union(usu).difference(unu)] for i in events][2:]\n",
    "\n",
    "al=set()\n",
    "for i in eve:\n",
    "    al=al.union(i[1])\n",
    "len(list(al)),len([i[0] for i in events])\n",
    "useful=\"'\"+\"','\".join([j[0] for j in eve])+\"'\"\n",
    "params=\"'\"+\"','\".join(al)+\"'\"\n",
    "usual=\"'\"+\"','\".join(list(usu))+\"'\"\n",
    "\n",
    "#分析数据库中的字段情况\n",
    "ana=\"\"\"\n",
    "SELECT event, a.k as k,count(distinct cast(a.v as bigint))as v,sum(cast(a.v as bigint))/count(a.v) as avg,min(cast(a.v as bigint)) as mn,max(cast(a.v as bigint))as ma\n",
    "FROM mobile_game.dwd_event_log_stage_hi\n",
    "LATERAL VIEW explode(params)a as k,v\n",
    "WHERE p_date='2020-12-21'\n",
    "AND app_id=2447\n",
    "AND app_id_str='2447'\n",
    "AND server_id=30016\n",
    "AND event IN({0})\n",
    "AND a.k IN({1})\n",
    "GROUP BY event, a.k\n",
    "UNION\n",
    "SELECT 'usual', a.k as k,count(distinct cast(a.v as bigint))as v,sum(cast(a.v as bigint))/count(a.v) as avg,min(cast(a.v as bigint)) as mn,max(cast(a.v as bigint))as ma\n",
    "FROM mobile_game.dwd_event_log_stage_hi\n",
    "LATERAL VIEW explode(params)a as k,v\n",
    "WHERE p_date='2020-12-21'\n",
    "AND app_id=2447\n",
    "AND app_id_str='2447'\n",
    "AND server_id=30016\n",
    "AND event IN({0})\n",
    "AND a.k IN({2})\n",
    "GROUP BY a.k\n",
    "\"\"\".format(useful,params,usual)\n",
    "\n",
    "pms_ana=spark.sql(ana).toPandas()\n",
    "\n",
    "pms_ana=pms_ana.query('avg>-999 and v>1')\n",
    "pms_ana.sort_values('event')\n",
    "\n",
    "spark.createDataFrame(pms_ana).write.format('hive').mode('overwrite').saveAsTable('oasis_p2_va.dim_p2_key_value')\n",
    "\n",
    "ana=\"\"\"\n",
    "SELECT * FROM oasis_p2_va.dim_p2_key_value\n",
    "\"\"\"\n",
    "\n",
    "pms_ana=spark.sql(ana).toPandas()\n",
    "\n",
    "pms_ana.query(\"event=='usual'\")\n",
    "\n",
    "#总结字段情况，在darado中即生成个人矩阵（非稀疏），notebook中不对个人矩阵再进行操作，否则速度极慢。但这种办法会导致大量数据冗余，导致数据量上限大幅降低\n",
    "pms1=pms_ana.sort_values('event').values.tolist()\n",
    "pms1=[i for i in pms1 if i[2]>1]\n",
    "public=[(n[1],(n[4],n[5])) for n in pms_ana.query(\"event=='usual'\").values.tolist()]\n",
    "private=[(i[1],(i[4],i[5]),i[0]) for i in pms1 if not i[1] in usu]\n",
    "public_str=','.join([\"(cast(params['{0}'] as bigint)-({1}))/({2}-({1}))\".format(i[0],i[1][0],i[1][1]) for i in public])\n",
    "private_str=','.join([\"if(event='{3}',(cast(params['{0}'] as bigint)-({1}))/({2}-({1})),0)\".format(i[0],i[1][0],i[1][1],i[2]) for i in private])\n",
    "params=\"array({0},{1})\".format(public_str,private_str)\n",
    "params\n",
    "\n",
    "#总结字段情况，准备生成sql语句，这是稀疏矩阵路线，后弃用，因为处理稀疏矩阵仍然耗时，keras也需要array输入而非稀疏阵\n",
    "pms1=pms_ana.sort_values('event').values.tolist()\n",
    "pms1=[i for i in pms1 if i[2]>1]\n",
    "iiter=0\n",
    "def itering():\n",
    "    global iiter\n",
    "    iiter+=1\n",
    "    return iiter\n",
    "public=[(n[1],(n[4],n[5])) for n in pms_ana.query(\"event=='usual'\").values.tolist()]\n",
    "public_str=','.join([\"{3},cast((cast(params['{0}'] as DECIMAL(22,1))-({1}))/{4} as float)\".format(i[0],i[1][0],i[1][1],itering(),i[1][1]-i[1][0]) for i in public])\n",
    "private=[(itering(),i[1],i[4],i[5],i[0]) for i in pms1 if not i[1] in usu]\n",
    "prev=''\n",
    "lst=[]\n",
    "lst1=[]\n",
    "for i in private:\n",
    "    if prev=='':prev=i[4]\n",
    "    lst.append(\"{0},cast((cast(params['{1}'] as DECIMAL(22,1))-({2}))/{3} as float)\".format(i[0],i[1],i[2],i[3]-i[2]))\n",
    "    if i[4]!=prev:\n",
    "        lst1.append(\"WHEN event='{0}' THEN map(\".format(prev)+public_str+','+','.join(lst)+\")\")\n",
    "        prev=i[4]\n",
    "        lst=[]\n",
    "pstr=' '.join(lst1)\n",
    "PARAMS=pstr\n",
    "\n",
    "#dim=iiter\n",
    "dim=180\n",
    "dim\n",
    "\n",
    "#读取训练数据\n",
    "sql=\"\"\"\n",
    "SELECT a.role_id, max(if(d.role_id is null,0,1))ret, collect_list(params) pms\n",
    "FROM(\n",
    "    SELECT role_id, p_date\n",
    "    FROM mobile_game.dwd_event_log_stage_hi\n",
    "    WHERE p_date BETWEEN '{2}' AND '{3}'\n",
    "    AND app_id=2447\n",
    "    AND app_id_str='2447'\n",
    "    AND event='create_role_success'\n",
    "    AND server_id>=30016\n",
    ")a\n",
    "INNER JOIN(\n",
    "    SELECT role_id,event,time, p_date,\n",
    "    {0} params,\n",
    "    row_number() over(\n",
    "        partition by role_id,p_date\n",
    "        order by time\n",
    "        desc\n",
    "    )rn\n",
    "    FROM mobile_game.dwd_event_log_stage_hi\n",
    "    WHERE p_date BETWEEN '{2}' AND '{3}'\n",
    "    AND app_id=2447\n",
    "    AND app_id_str='2447'\n",
    "    AND server_id>=30016\n",
    "    AND event IN({1})\n",
    ")b\n",
    "ON a.role_id=b.role_id AND a.p_date=b.p_date\n",
    "LEFT JOIN(\n",
    "    SELECT role_id\n",
    "    FROM mobile_game.dwd_event_log_stage_hi\n",
    "    WHERE p_date BETWEEN '{4}' AND '{5}'\n",
    "    AND app_id=2447\n",
    "    AND app_id_str='2447'\n",
    "    AND event='retention'\n",
    "    AND server_id>=30016\n",
    "    AND params['day']='1'\n",
    "    GROUP BY role_id\n",
    ")d\n",
    "ON a.role_id=d.role_id\n",
    "WHERE rn<1200\n",
    "GROUP BY a.role_id\n",
    "\"\"\".format(params,useful,'2020-12-21','2021-01-26','2020-12-22','2021-01-27')\n",
    "\"\"\"这个代码在dorado分日期运行更快（不要在这里运行这个代码），这里只是sql代码生成，把p_date的范围改为${DATE}\"\"\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%hql\n",
    "SELECT *\n",
    "FROM oasis_p2_va.dwd_p2_samples_di\n",
    "WHERE p_date='${DATE}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list=_\n",
    "\n",
    "#样本量\n",
    "len(date_list)"
   ]
  },
  {
   "source": [
    "plt.scatter([len(i[1].pms) for i in dfs.iterrows()],[i[1].ret for i in dfs.iterrows()],s=0.1)\n",
    "\n",
    "tot=dfs.shape[0]\n",
    "cnt=0\n",
    "samples=[]\n",
    "#list-dictionary to sparse matrix\n",
    "def ld2sm(ld):\n",
    "    ld=ld[1].pms\n",
    "    global cnt,tot\n",
    "    mat=lil_matrix((1000,180))\n",
    "    for i in range(1000):\n",
    "        j=999-i\n",
    "        if i==len(ld):break\n",
    "        for k in ld[i]:\n",
    "            mat[j,k]=ld[i][k]\n",
    "            if not mat[j,k]>-999:mat[j,k]=0\n",
    "    if floor(100*len(samples)/tot)!=floor(100*(len(samples)+1)/tot):print(floor(100*(len(samples)+1)/tot),flush=True,end=' ')\n",
    "    samples.append(mat)\n",
    "\n",
    "from multiprocessing.dummy import Pool\n",
    "pool=Pool()\n",
    "pool.map(print,[1,2,3,4,list(range(10000)),6,7,8])\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "sample0=[[(i[1].pms,i[1].ret) for i in j[2].iterrows()] for j in date_list if j[2] is not None]\n",
    "samples=[]\n",
    "y=[]\n",
    "zero=[0 for i in range(180)]\n",
    "for i in sample0:\n",
    "    for j in i:\n",
    "        j[0].reverse()\n",
    "        if len(j[0])<1199:\n",
    "            a=[zero for i in range(1199-len(j[0]))]\n",
    "            a.extend(j[0])\n",
    "            samples.append(a)\n",
    "        else:samples.append(j[0])\n",
    "        y.append(j[1])\n",
    "\n",
    "len(samples)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=180\n",
    "sample0=[(i[1].pms,i[1].ret) for i in date_list.iterrows()]\n",
    "samples=[]\n",
    "y=[]\n",
    "zero=[0 for i in range(180)]\n",
    "for j in sample0:\n",
    "    j[0].reverse()\n",
    "    if len(j[0])<1199:\n",
    "        a=[zero for i in range(1199-len(j[0]))]\n",
    "        a.extend(j[0])\n",
    "        samples.append(a)\n",
    "    else:samples.append(j[0])\n",
    "    y.append(j[1])\n",
    "        \n",
    "#模型与损失\n",
    "def create_model(sequence_length, converter=10, units=64, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"binary_crossentropy\", optimizer=\"rmsprop\"):\n",
    "    model = Sequential()\n",
    "    #model.add(Input(shape=(sequence_length,dim),sparse=True))\n",
    "    #model.add(Embedding(dim+3, converter))\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            model.add(cell(units, return_sequences=True,input_shape=(sequence_length,dim)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=loss, metrics=[\"binary_accuracy\"], optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, val_data, batch_size = 20):\n",
    "        super().__init__()\n",
    "        self.validation_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(\n",
    "            self.validation_data[0]))).round().astype(int)\n",
    "        val_targ = self.validation_data[1].astype(int)\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        _val_confusion = confusion_matrix(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(_val_confusion)\n",
    "\n",
    "        return\n",
    "    \n",
    "#参数\n",
    "N_STEPS = None\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 128\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "\n",
    "\n",
    "LOSS = \"binary_crossentropy\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "\n",
    "ret=[]\n",
    "los=[]\n",
    "for i in range(len(y)):\n",
    "    if y[i]==1:ret.append(i)\n",
    "    else:los.append(i)\n",
    "los=los[:len(ret)]\n",
    "ret.extend(los)\n",
    "rd.shuffle(ret)\n",
    "\n",
    "model = create_model(N_STEPS, 40, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=keras.optimizers.Adam(clipvalue=0.5,lr=0.001))\n",
    "cut=round(len(ret)*0.1)\n",
    "v_dat=ret[:cut]\n",
    "t_dat=ret[cut:]\n",
    "\n",
    "X_train=np.asarray([samples[i] for i in t_dat])\n",
    "Y_train=np.asarray([y[i] for i in t_dat])\n",
    "X_val=np.asarray([samples[i] for i in v_dat])\n",
    "Y_val=np.asarray([y[i] for i in v_dat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "\n",
    "metrics=Metrics((X_val,Y_val),batch_size=BATCH_SIZE)\n",
    "history=model.fit(X_train,Y_train,initial_epoch=0,epochs=50,batch_size=BATCH_SIZE,shuffle=True,\n",
    "          validation_data=(X_val,Y_val),callbacks=[metrics])\n",
    "\n",
    "print(metrics.val_precisions)\n",
    "print(metrics.val_recalls)\n",
    "import time\n",
    "model.save('loss{0}.h5'.format(time.time()))"
   ]
  }
 ]
}